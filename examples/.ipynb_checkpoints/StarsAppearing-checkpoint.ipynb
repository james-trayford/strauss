{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03b73a45",
   "metadata": {},
   "source": [
    "### <u> Generate the \"_Stars Appearing_\" sonification used in the \"_Audible Universe_\" planetarium show </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f21355",
   "metadata": {},
   "source": [
    "First, we import relevant modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64e736a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload \n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "import ffmpeg as ff\n",
    "import wavio as wav\n",
    "from strauss.sonification import Sonification\n",
    "from strauss.sources import Events\n",
    "from strauss import channels\n",
    "from strauss.score import Score\n",
    "import numpy as np\n",
    "from strauss.generator import Sampler\n",
    "import IPython.display as ipd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb93a2a6",
   "metadata": {},
   "source": [
    "<u> __The Score:__ </u>\n",
    "\n",
    "We set up the ***Score***; this is analagous to a musical score and controls what notes can be played over the course of the sonification. We can specify a chord sequence as a single string (`str`) where chord names are separated by a `|` character. The root octave of the chord may also be specified by adding `_X` where `X` is the octave number. <span style=\"color:gray\">_(Note: for now, each chord occupies an equal lenth in the sonification, in the future chord change times can be directly specified and optionally related to events in the data)_</span>\n",
    "\n",
    "We can directly specify the ___chord voicing___ as \n",
    "a list of lists containing the notes from low to high in each chord.\n",
    "\n",
    "Here, we  are directly specifying a single __`Db6/9`__ chord voicing. These notes will later be played by stars of different colours!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f14379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chords = [['Db3','Gb3', 'Ab3', 'Eb4','F4']]\n",
    "length = \"1m 30s\"\n",
    "score =  Score(chords, length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09aa39a",
   "metadata": {},
   "source": [
    "<u> __The Sources:__ </u>\n",
    "\n",
    "Next, we import the data that will represent ___sources___ of sound. The data is the sky positions, brightness and colour of stars from the _Paranal_ observatory site in _Chile_. This data is contained in an `ascii` file (specified by the `datafile` variable) and organised where each __row__ is a star and each __column__ is a property of that star.\n",
    "\n",
    "The idea here is that as night draws in we see the brightet stars first. As it gets darker, and our eyes adjust, we see more dim stars. We \"***sonify***\" this by having a note play when each star appears. The \"***panning***\" (a.k.a stereo imaging) of the note is controlled by the ***altitude*** and ***azimuth*** of the stars, as if we were facing south. The ***colour*** of the star contols the note within the chord we've chosen, where notes low to high (short to long wavelength) represent fixed-number bins in colour from blue to red (again, short to long wavelength). Finally, the volume of the note is also related to the brightness of the star (dimmer stars are quieter). This is chosen to give a relatively even volume throught the sonification, as dim stars are much more numerous than bright ones <span style=\"color:gray\">_(Note: in the future we will have the option to scale volumes in this way automatically)_</span>\n",
    "\n",
    "We speciify the sound preperty to star property mappimg as as three `dict` objects with keys representing each sound property we dapat in the sonification:\n",
    "- **`mapcols`**: entries are the data file columns used to map each property\n",
    "- **`mapvals`**: entries are function objects that manipulate each columns values to yield the linear mapping\n",
    "- **`maplims`**: entries are `tuple`s representing the (`low`,`high`) limits of each mapping.numerical values represent absolute limits (used here for the angles in degrees to correctly limit the `phi` and `theta` mappings to 360° (2π) and 180° (π) respectively. `str` values are taken to be percentiles from 0 to 100. string values > 100 can also be used, where e.g. 104 is 4% larger than the 100th percentile value. This is used for the time here, so that the last sample doesnt trigger at exactly the end of the sonification, giving time for the sound to die away slowly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dd6b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = \"../data/datasets/stars_paranal.txt\"\n",
    "mapcols =  {'phi':1, 'theta':0, 'volume':2, 'time':2, 'pitch':3}\n",
    "\n",
    "mapvals =  {'phi': lambda x : x,\n",
    "            'theta': lambda x : 90.-x,\n",
    "            'time': lambda x : x,\n",
    "            'pitch' : lambda x: -x,\n",
    "            'volume' : lambda x : (1+np.argsort(x).astype(float))**-0.2}\n",
    "\n",
    "maplims =  {'phi': (0, 360),\n",
    "            'theta': (0, 180), \n",
    "            'time': ('0', '104'),\n",
    "            'pitch' : ('0', '100'),\n",
    "            'volume' : ('0', '100')}\n",
    "\n",
    "events = Events(mapcols.keys())\n",
    "events.fromfile(datafile, mapcols)\n",
    "events.apply_mapping_functions(mapvals, maplims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be56264",
   "metadata": {},
   "source": [
    "<u> __The Generator:__ </u>\n",
    "\n",
    "The final element we need is a ***Generator*** that actually generates the audio given the ***Score*** and ***Sources***. Here, we use a ***Sampler***-type generator that plays an audio sample for each note. The samples and other parameters (not specified here) control the sound for each note. These can be specified in `dict` format note-by-note (keys are note name strings, entries are strings pointing to the `WAV` format audio sample to load) or just using a string that points to a sample directory (each sample filename in that directory ends with `_XX.wav` where `XX` is the note name) we use the example sample back in `./data/samples/glockenspiels` <span style=\"color:gray\">_(Note: rendering can take a while with the long audio samples we use here, shorter samples can be used to render faster, such as those in `./data/samples/mallets`. This is also useful if you want to try different notes or chords, as only the 5 notes specified above are provided in the glockenspiel sample folder.)_</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2336d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = Sampler(\"../data/samples/glockenspiels\")\n",
    "sampler.preset_details(\"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cc7dfd",
   "metadata": {},
   "source": [
    "<u> __The Sonification:__ </u>\n",
    "\n",
    "We consolidate the three elements above in to a sonification object to generate the sound, specifying the audio setup (here `'stereo'` as opposed to `'mono'`, `'5.1'`, etc). <span style=\"color:gray\">_(Note: you can generate the audio in any specified audio setup, but following cells assume stereo and only mono and stereo formats are supported by the jupyter audio player in the final cell)_</span>\n",
    "\n",
    "We then `render` the sonification to generate the audio track (may take some time with the glockenspiel samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdf42a0-96a5-45c1-b060-ad9a720fb7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"stereo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9095040",
   "metadata": {},
   "outputs": [],
   "source": [
    "soni = Sonification(score, events, sampler, system)\n",
    "soni.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3784e811",
   "metadata": {},
   "source": [
    "Finally, let's visualise the waveform, and preview the audio in-notebook*!\n",
    "\n",
    "<span style=\"color:gray\">_*if using a surround sound format (i.e > 2 channels) the preview is stereo, with the first two channels mapped left and right, due to the limitations of the notebook audio player_</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f5a383",
   "metadata": {},
   "outputs": [],
   "source": [
    "soni.notebook_display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7409e2c-3357-4385-9488-f818e5e4cfde",
   "metadata": {},
   "source": [
    "Run `soni.save_combined('<directory/to/filename.wav>')` if you want to save the sonification to a file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564605db-81a9-4f6c-9437-29ee466b5147",
   "metadata": {},
   "outputs": [],
   "source": [
    "soni.save_combined('../../rendered_stars_5p1.wav',True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
